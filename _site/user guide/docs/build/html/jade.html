
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Joint Academic Data Science Endeavour - running Jobs on the JADE system &#8212; JADE 0.1 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to JADE’s documentation!" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="joint-academic-data-science-endeavour-running-jobs-on-the-jade-system">
<h1>Joint Academic Data Science Endeavour - running Jobs on the JADE system<a class="headerlink" href="#joint-academic-data-science-endeavour-running-jobs-on-the-jade-system" title="Permalink to this headline">¶</a></h1>
<p>Last modified: 28/7/2017</p>
<p>The whole system from the user perspective is interacted with via the Slurm Workload Manager on the login nodes. Via this scheduler, access to the compute nodes, can be interactive or batch. The installed application software consists of a mixture of docker container images, supplied by Nvidia, and executables built from source. Both container images and executables can use the system either interactively or in batch mode.</p>
<p>It is only possible to ssh onto a node which has been allocated to the user. Once the session completes the ssh access is removed. Access to the global parallel file system is from the login nodes and all compute nodes. Any data on this file system is retained after a session on the nodes completes. There is also access to local disc space on each node. Access to this file system is only possible during a Slurm session. Once the session completes the local disc data is removed.</p>
<p>The software initially installed on the machine is listed in the following table:</p>
<table border="1" class="colwidths-given docutils">
<colgroup>
<col width="40%" />
<col width="40%" />
<col width="20%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Application</th>
<th class="head">Version</th>
<th class="head">Note</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>GNU compiler suite</td>
<td>4.8.4</td>
<td>part of O/S</td>
</tr>
<tr class="row-odd"><td>PGI compiler suite</td>
<td>17.4</td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td>OpenMPI</td>
<td>1.10.2</td>
<td>Supplied with PGI</td>
</tr>
<tr class="row-odd"><td>OpenMPI</td>
<td>1.10.5a1</td>
<td>Supplied with PGI</td>
</tr>
<tr class="row-even"><td>Gromacs</td>
<td>2016.3</td>
<td>Supplied by Nvidia</td>
</tr>
<tr class="row-odd"><td>NAMD</td>
<td>2.12</td>
<td>&#160;</td>
</tr>
</tbody>
</table>
<p>This software has been built from source and installed as modules. To list the source built applications do:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ module avail
----------------------------------------------------- /jmain01/apps/modules -----------------------------
gromacs/2016.3           openmpi/1.10.2/2017      pgi/17.4(default)        pgi64/17.4(default)
PrgEnv-pgi/17.4(default)      NAMD/2.12      openmpi/1.10.5a1/GNU     pgi/2017             pgi64/2017
</pre></div>
</div>
<p>The applications initially supplied by Nvidia as containers are listed in the following table:</p>
<table border="1" class="colwidths-given docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Application</th>
<th class="head">Version</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Caffe</td>
<td>17.04</td>
</tr>
<tr class="row-odd"><td>Theano</td>
<td>17.04</td>
</tr>
<tr class="row-even"><td>Torch</td>
<td>17.04</td>
</tr>
</tbody>
</table>
<p>To list the containers and version available on the system do:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ containers
REPOSITORY                    TAG                 IMAGE ID                CREATED               SIZE
nvidia/cuda                  latest              15e5dedd88c5        4 weeks ago          1.67 GB
nvcr.io/nvidia/caffe         17.04               87c288427f2d        6 weeks ago          2.794 GB
nvcr.io/nvidia/theano        17.04               24943feafc9b         8 weeks ago          2.386 GB
nvcr.io/nvidia/torch         17.04               a337ffb42c8e         9 weeks ago          2.9 GB
</pre></div>
</div>
<p>The following brief notes explain how run the various applications.</p>
<div class="section" id="gromacs">
<h2>Gromacs<a class="headerlink" href="#gromacs" title="Permalink to this headline">¶</a></h2>
<p>The latest version of the source was used. The following build instructions were followed: <a class="reference external" href="http://www.nvidia.com/object/gromacs-installation.html">http://www.nvidia.com/object/gromacs-installation.html</a></p>
<p>The code was compiled using OpenMPI v1.10.5a1 and GCC v4.8.4 with the following command:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">CC</span><span class="o">=</span><span class="n">mpicc</span> <span class="n">CXX</span><span class="o">=</span><span class="n">mpicxx</span> <span class="n">cmake</span> <span class="o">/</span><span class="n">jmain01</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">atostest</span><span class="o">/</span><span class="n">Building</span><span class="o">/</span><span class="n">gromacs</span><span class="o">-</span><span class="mf">2016.3</span>
<span class="o">-</span><span class="n">DGMX_OPENMP</span><span class="o">=</span><span class="n">ON</span>
<span class="o">-</span><span class="n">DGMX_GPU</span><span class="o">=</span><span class="n">ON</span>
<span class="o">-</span><span class="n">DGPU_DEPLOYMENT_KIT_ROOT_DIR</span><span class="o">=/</span><span class="n">usr</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">cuda</span><span class="o">-</span><span class="mf">8.0</span><span class="o">/</span><span class="n">targets</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span>
<span class="o">-</span><span class="n">DCUDA_TOOLKIT_ROOT_DIR</span><span class="o">=/</span><span class="n">usr</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">cuda</span><span class="o">-</span><span class="mf">8.0</span><span class="o">/</span><span class="n">targets</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span>
<span class="o">-</span><span class="n">DNVML_INCLUDE_DIR</span><span class="o">=/</span><span class="n">usr</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">cuda</span><span class="o">-</span><span class="mf">8.0</span><span class="o">/</span><span class="n">targets</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">/</span><span class="n">include</span>
<span class="o">-</span><span class="n">DNVML_LIBRARY</span><span class="o">=/</span><span class="n">usr</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">nvidia</span><span class="o">-</span><span class="mi">375</span><span class="o">/</span><span class="n">libnvidia</span><span class="o">-</span><span class="n">ml</span><span class="o">.</span><span class="n">so</span>
<span class="o">-</span><span class="n">DHWLOC_INCLUDE_DIRS</span><span class="o">=/</span><span class="n">usr</span><span class="o">/</span><span class="n">mpi</span><span class="o">/</span><span class="n">gcc</span><span class="o">/</span><span class="n">openmpi</span><span class="o">-</span><span class="mf">1.10</span><span class="o">.</span><span class="mi">5</span><span class="n">a1</span><span class="o">/</span><span class="n">include</span><span class="o">/</span><span class="n">openmpi</span><span class="o">/</span><span class="n">opal</span><span class="o">/</span><span class="n">mca</span><span class="o">/</span><span class="n">hwloc</span><span class="o">/</span><span class="n">hwloc191</span><span class="o">/</span><span class="n">hwloc</span><span class="o">/</span><span class="n">include</span>
<span class="o">-</span><span class="n">DGMX_BUILD_OWN_FFTW</span><span class="o">=</span><span class="n">ON</span>
<span class="o">-</span><span class="n">DGMX_PREFER_STATIC_LIBS</span><span class="o">=</span><span class="n">ON</span>
<span class="o">-</span><span class="n">DCMAKE_BUILD_TYPE</span><span class="o">=</span><span class="n">Release</span>
<span class="o">-</span><span class="n">DGMX_BUILD_UNITTESTS</span><span class="o">=</span><span class="n">ON</span>
<span class="o">-</span><span class="n">DCMAKE_INSTALL_PREFIX</span><span class="o">=/</span><span class="n">jmain01</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">atostest</span><span class="o">/</span><span class="n">gromacs</span><span class="o">-</span><span class="mf">2016.3</span>
</pre></div>
</div>
<p>The following is an example Slurm script to run the code using one of the regression tests from the installation:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes=2</span>
<span class="c1">#SBATCH --ntasks=4</span>
<span class="c1">#SBATCH -p all</span>
<span class="c1">#SBATCH -J Gromacs</span>
<span class="c1">#SBATCH --time=01:00:00</span>
<span class="c1">#SBATCH --gres=gpu:2</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">gromacs</span><span class="o">/</span><span class="mf">2016.3</span>
<span class="o">./</span><span class="n">gmxtest</span><span class="o">.</span><span class="n">pl</span> <span class="o">-</span><span class="n">np</span> <span class="mi">8</span> <span class="o">-</span><span class="n">verbose</span> <span class="n">simple</span>
</pre></div>
</div>
<p>As can be seem the code will run across multiple compute nodes. Also, the number of GPUs per node is requested using the <code class="docutils literal"><span class="pre">gres</span></code> option. In this example the user will request 2 out of the possible 8 on the nodes.</p>
<p>There is access to the local disc space on each node whilst the batch session is in progress. It can be accessed via: <code class="docutils literal"><span class="pre">/raid/local_scratch/$USERID</span></code></p>
<p>Any data within the directory will be lost once the session completes.</p>
</div>
<div class="section" id="namd">
<h2>NAMD<a class="headerlink" href="#namd" title="Permalink to this headline">¶</a></h2>
<p>The latest version of the source code was used and built using OpenMPI v1.10.5a1 and GCC v4.8.4 following instructions from: <a class="reference external" href="http://www.nvidia.com/object/gpu-accelerated-applications-namd-installation.html">http://www.nvidia.com/object/gpu-accelerated-applications-namd-installation.html</a></p>
<p>Charm++ was built using:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">build</span> <span class="n">charm</span><span class="o">++</span> <span class="n">verbs</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">x86_64</span> <span class="n">gcc</span> <span class="n">smp</span> <span class="o">--</span><span class="k">with</span><span class="o">-</span><span class="n">production</span>
</pre></div>
</div>
<p>NAMD was built using the following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">config</span> <span class="n">Linux</span><span class="o">-</span><span class="n">x86_64</span><span class="o">-</span><span class="n">g</span><span class="o">++</span> <span class="o">--</span><span class="n">charm</span><span class="o">-</span><span class="n">arch</span> <span class="n">verbs</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">x86_64</span><span class="o">-</span><span class="n">smp</span><span class="o">-</span><span class="n">gcc</span> <span class="o">--</span><span class="k">with</span><span class="o">-</span><span class="n">cuda</span> <span class="o">--</span><span class="n">cuda</span><span class="o">-</span><span class="n">prefix</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">cuda</span><span class="o">-</span><span class="mf">8.0</span>
</pre></div>
</div>
<p>For the decision on the number of threads to use per node, take a look at: <a class="reference external" href="http://www.nvidia.com/object/gpu-accelerated-applications-namd-running-jobs.html">http://www.nvidia.com/object/gpu-accelerated-applications-namd-running-jobs.html</a></p>
<div class="highlight-default"><div class="highlight"><pre><span></span>#!/bin/bash
#SBATCH --nodes=2
#SBATCH -p all
#SBATCH -J NAMD
#SBATCH --time=01:00:00
#SBATCH --gres=gpu:8

module load NAMD/2.12

#set up the nodelist file
srun hostname &gt; hf-1
sed &#39;s/^/host /&#39; hf-1 &gt; hf &amp;&amp; rm hf-1
echo &#39;group main ++shell ssh&#39; | cat - hf  &gt; hf-1 &amp;&amp; mv hf-1 hf

$NAMDROOT/charmrun ++p 64 ++ppn 4 $NAMDROOT/namd2
++nodelist hf +setcpuaffinity +pemap 0-19,20-39 +commap 0,20
+devices 3,1 src/alanin

rm hf
</pre></div>
</div>
<p>As can be seen from the script setting above the code will run across multiple nodes, in this case 2 nodes.</p>
</div>
<div class="section" id="using-containerised-applications">
<h2>Using Containerised Applications<a class="headerlink" href="#using-containerised-applications" title="Permalink to this headline">¶</a></h2>
<p>On entering the container the present working directory will be the user’s home directory: <code class="docutils literal"><span class="pre">/home_directory</span></code></p>
<p>Any files you copy into <code class="docutils literal"><span class="pre">/home_directory</span></code> will have the same userid as normal and will be available once exiting the container. The local disk space on the node is available at: <code class="docutils literal"><span class="pre">/local_scratch/$USERID</span></code></p>
<p>This is 6.6TB in size but any data will be lost once the interactive session is ended. There are two ways of interacting with the containerised applications.</p>
<div class="section" id="interactive-mode">
<h3>1.Interactive Mode<a class="headerlink" href="#interactive-mode" title="Permalink to this headline">¶</a></h3>
<p>All the applications in containers can be launched interactively in the same way using 1 compute node at a time. The number of GPUs to be used per node is requested using the <code class="docutils literal"><span class="pre">gres</span></code> option. To request an interactive session on a compute node the following command is issued from the login node:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">srun</span> <span class="o">--</span><span class="n">gres</span><span class="o">=</span><span class="n">gpu</span><span class="p">:</span><span class="mi">2</span> <span class="o">--</span><span class="n">pty</span> <span class="o">/</span><span class="n">jmain01</span><span class="o">/</span><span class="n">apps</span><span class="o">/</span><span class="n">docker</span><span class="o">/</span><span class="n">caffe</span> <span class="mf">17.04</span>
</pre></div>
</div>
<p>This command will show the following, which is now running on a compute node:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>================
==NVIDIA Caffe==
================

NVIDIA Release 17.04 (build 26740)

Container image Copyright (c) 2017, NVIDIA CORPORATION.  All rights reserved.
Copyright (c) 2014, 2015, The Regents of the University of California (Regents)
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.
NVIDIA modifications are covered by the license terms that apply to the underlying project or file.

groups: cannot find name for group ID 1002
I have no name!@124cf0e3582e:/home_directory$
</pre></div>
</div>
<p>Note. The warnings in the last two lines can be ignored. To exit the container, issue the “exit” command. To launch the other containers the commands are:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">srun</span> <span class="o">--</span><span class="n">gres</span><span class="o">=</span><span class="n">gpu</span><span class="p">:</span><span class="mi">8</span> <span class="o">--</span><span class="n">pty</span> <span class="o">/</span><span class="n">jmain01</span><span class="o">/</span><span class="n">apps</span><span class="o">/</span><span class="n">docker</span><span class="o">/</span><span class="n">theano</span> <span class="mf">17.04</span>
<span class="n">srun</span> <span class="o">--</span><span class="n">gres</span><span class="o">=</span><span class="n">gpu</span><span class="p">:</span><span class="mi">4</span> <span class="o">--</span><span class="n">pty</span> <span class="o">/</span><span class="n">jmain01</span><span class="o">/</span><span class="n">apps</span><span class="o">/</span><span class="n">docker</span><span class="o">/</span><span class="n">torch</span> <span class="mf">17.04</span>
</pre></div>
</div>
</div>
<div class="section" id="batch-mode">
<h3>2.Batch Mode<a class="headerlink" href="#batch-mode" title="Permalink to this headline">¶</a></h3>
<p>There are wrappers for launching the containers in batch mode. For example, to launch the Torch application change directory to where the launching script is, in this case called <code class="docutils literal"><span class="pre">submit-char.sh</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">/</span><span class="n">jmain01</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">atostest</span><span class="o">/</span><span class="n">char</span><span class="o">-</span><span class="n">rnn</span><span class="o">-</span><span class="n">master</span>
</pre></div>
</div>
<p>A Slurm batch script is used to launch the code, such as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH -p all</span>
<span class="c1">#SBATCH -J Torch</span>
<span class="c1">#SBATCH --gres=gpu:8</span>
<span class="c1">#SBATCH --time=01:00:00</span>

<span class="o">/</span><span class="n">jmain01</span><span class="o">/</span><span class="n">apps</span><span class="o">/</span><span class="n">docker</span><span class="o">/</span><span class="n">torch</span><span class="o">-</span><span class="n">batch</span> <span class="o">-</span><span class="n">c</span> <span class="o">./</span><span class="n">submit</span><span class="o">-</span><span class="n">char</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>The output will appear in the slurm standard output file.</p>
<p>Each of the containerised applications has its own batch launching script:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">jmain01</span><span class="o">/</span><span class="n">apps</span><span class="o">/</span><span class="n">docker</span><span class="o">/</span><span class="n">torch</span><span class="o">-</span><span class="n">batch</span>
<span class="o">/</span><span class="n">jmain01</span><span class="o">/</span><span class="n">apps</span><span class="o">/</span><span class="n">docker</span><span class="o">/</span><span class="n">caffe</span><span class="o">-</span><span class="n">batch</span>
<span class="o">/</span><span class="n">jmain01</span><span class="o">/</span><span class="n">apps</span><span class="o">/</span><span class="n">docker</span><span class="o">/</span><span class="n">theano</span><span class="o">-</span><span class="n">batch</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="more-information">
<h2>More Information<a class="headerlink" href="#more-information" title="Permalink to this headline">¶</a></h2>
<p>JADE Web site: <a class="reference external" href="http://www.arc.ox.ac.uk/content/jade">http://www.arc.ox.ac.uk/content/jade</a></p>
<p>Mike Giles’ Web site: <a class="reference external" href="http://people.maths.ox.ac.uk/~gilesm/JADE/">http://people.maths.ox.ac.uk/~gilesm/JADE/</a></p>
<p>STFC Web site: <a class="reference external" href="https://www.hartree.stfc.ac.uk/Pages/Hartree-Centre-welcomes-new-HPC-computing-facility-to-support-machine-learning.aspx">https://www.hartree.stfc.ac.uk/Pages/Hartree-Centre-welcomes-new-HPC-computing-facility-to-support-machine-learning.aspx</a></p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">JADE</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Joint Academic Data Science Endeavour - running Jobs on the JADE system</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#gromacs">Gromacs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#namd">NAMD</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-containerised-applications">Using Containerised Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="#more-information">More Information</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Welcome to JADE’s documentation!</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Mozhgan K Chimeh.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="_sources/jade.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>